% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[graybox]{svmult}

\documentclass{article}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage[margin=1in]{geometry}

%\usepackage{mathptmx}       % selects Times Roman as basic font
%\usepackage{helvet}         % selects Helvetica as sans-serif font
%\usepackage{courier}        % selects Courier as typewriter font
%\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom

\usepackage[authoryear,round]{natbib}

\usepackage{listings}
% see the list of further useful packages
% in the Reference Guide
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{colortbl}

\usepackage{authblk}
\usepackage{url}

\usepackage[fleqn]{amsmath}
\setlength{\mathindent}{0em}

%\usepackage{mathpazo}
%\usepackage[scaled=.95]{helvet}
%\renewcommand\familydefault{\sfdefault}

\renewcommand\arraystretch{1.2}

\pagestyle{empty}

\newcommand{\hdr}[2]{%
\textbf{\makebox[0pt]{\hspace{5mm}#1}\hspace{0.5\linewidth}\makebox[0pt][c]{#2}}%
}

\newcommand{\Nvp}{N_{\text{vp}}}

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program



\begin{document}
\lstset{% Settings for the listing package
  language=Python,
  numbers=left,
  basicstyle=\small,
  showstringspaces=false,
  lineskip=2pt,
  escapechar=!,
  morekeywords={Create,SetStatus,GetStatus,Connect,
     ConvergentConnect,DivergentConnect,
     Simulate,RandomConvergentConnect,
     RandomDivergentConnect,CopyModel,
     GetKernelStatus,SetKernelStatus,PrintNetwork,SetDefaults,
   voltage_trace,raster_plot,ResetKernel,GetConnections},
  rangeprefix=\#\ **\ ,
  rangesuffix=\ **,
  includerangemarker=false}

\title{NEST by Example: An Introduction to the 
\mbox{Neural Simulation Tool NEST Version 2.6.0}}
\author[1]{Marc-Oliver Gewaltig}
\author[2]{Abigail Morrison}
\author[3, 2]{Hans Ekkehard Plesser}
\affil[1]{Blue Brain Project, Ecole Polytechnique Federale de
  Lausanne, QI-J, Lausanne 1015, Switzerland}
\affil[2]{Institute of Neuroscience and Medicine (INM-6)
Functional Neural Circuits Group, J\"ulich Research Center, 52425
J\"ulich, Germany}
\affil[3]{Dept of Mathematical Sciences and Technology, 
Norwegian University of Life Sciences, PO Box 5003, 1432 Aas,
Norway}

\date{}

\maketitle

\abstract{The neural simulation tool NEST can simulate small to very
  large networks of point-neurons or neurons with a few
  compartments. In this chapter, we show by example how models are
  programmed and simulated in NEST.

  This document is based on a preprint version of
  \citet{Gewa:2012(533)} and has been updated for NEST 2.6 (r11744).

\begin{description}
\item[Updated to 2.6.0] Hans E.\ Plesser, December 2014
\item[Updated to 2.4.0] Hans E.\ Plesser, June 2014
\item[Updated to 2.2.2] Hans E.\ Plesser \& Marc-Oliver Gewaltig,
  December 2012
\end{description}
}

\section{Introduction}
\label{nest:sec:introduction}
NEST\index{NEST} is a simulator for networks of point neurons, that is, neuron
models that collapse the morphology (geometry) of dendrites, axons,
and somata into either a single compartment or a small number of
compartments \citep{Gewaltig2007}. This simplification is useful for
questions about the dynamics of large neuronal networks with complex
connectivity. In this text, we give a practical introduction to neural
simulations with NEST. We describe how network models are defined and
simulated, how simulations can be run in parallel, using multiple
cores or computer clusters, and how parts of a model can be
randomized.

The development of NEST started in 1994 under the name SYNOD\index{SYNOD} to
investigate the dynamics of a large cortex model, using
integrate-and-fire neurons \citep{SYNOD}. At that time the only
available simulators were NEURON \citep{Hine:1997(1179)} and GENESIS
\citep{Bower95a}, both focussing on morphologically detailed neuron
models, often using data from microscopic reconstructions.

Since then, the simulator has been under constant development. In
2001, the Neural Simulation Technology Initiative was founded to
disseminate our knowledge of neural simulation technology. The
continuing research of the member institutions into algorithms for the
simulation of large spiking networks has resulted in a number of
influential publications. The algorithms and techniques developed are
not only implemented in the NEST simulator, but have also found their
way into other prominent simulation projects, most notably the NEURON
simulator (for the Blue Brain Project: \citealp{Migliore06_119}) and
IBM's C2 simulator \citep{Ananthanarayanan09}.

Today, in 2012, there are several simulators for large spiking
networks to choose from \citep{Brette2007}, but NEST remains the
best established simulator with the the largest developer
community.
  
A NEST simulation consists of three main components: 
\begin{description}
\item[\bf Nodes] Nodes\index{NEST!node} are all neurons, devices, and also
  sub-networks. Nodes have a dynamic state that changes over time and
  that can be influenced by incoming \emph{events}.
\item[\bf Events] Events\index{NEST!event} are pieces of information of a particular
  type. The most common event is the spike-event. Other event types
  are voltage events and current events.
\item[\bf Connections] Connections\index{NEST!connection} are communication channels between
  nodes. Only if one node is connected to another node, can they
  exchange events. Connections are weighted, directed, and specific to
  one event type. Directed means that events can flow only in one
  direction. The node that sends the event is called \emph{source} and
  the node that receives the event is called \emph{target}. The weight
  determines how strongly an event will influence the target node. A
  second parameter, the \emph{delay}, determines how long an event
  needs to travel from source to target.
\end{description}

In the next sections, we will illustrate how to use NEST, using
examples with increasing complexity. Each of the examples is
self-contained. We suggest that you try each example, by typing it
into Python, line by line. Additionally, you can find all examples in
your NEST distribution.

\section{First steps}

We begin by starting Python. For interactive sessions, we recommend
the IPython shell \citep{Pere:2007(21)}. It is convenient,
because you can edit the command line and access previously typed
commands using the up and down keys. However, all examples in this
chapter work equally well without IPython. For data analysis and
visualization, we also recommend the Python packages Matplotlib
\citep{Hunt:2007(90)} and NumPy \citep{Olip:Guid}.

Our first simulation investigates the response of one
integrate-and-fire neuron to an alternating current and Poisson spike
trains from an excitatory and an inhibitory source. We record the
membrane potential of the neuron to observe how the stimuli influence
the neuron (see Fig.~\ref{nest:fig:voltagetrace}).

\begin{figure}[!tbp]
\centering
\includegraphics[width=0.7\linewidth]{figures/voltage_trace.eps}
\caption{\label{nest:fig:voltagetrace} Membrane potential of a neuron
  in response to an alternating current as well as random excitatory
  and inhibitory spike events. The membrane potential roughly follows
  the injected sine current. The small deviations from the sine curve are
  caused by the excitatory and inhibitory spikes that arrive at random
  times. Whenever the membrane potential reaches the firing threshold
  at -55 mV, the neuron spikes and the membrane potential is reset to
  -70 mV. In this example this happens twice: once at around 110 ms
  and again at about 600 ms.}
\end{figure}

In this model, we inject a sine current with a frequency of 2 Hz and
an amplitude of 100 pA into a neuron. At the same time, the neuron
receives random spiking input from two sources known as Poisson
generators. One Poisson generator represents a large population of
excitatory neurons and the other a population of inhibitory
neurons. The rate for each Poisson generator is set as the product of
the assumed number of neurons in a population and their average firing
rate.

The small network is simulated for 1000 milliseconds, after which the
time course of the membrane potential during this period is plotted
(see Fig.~\ref{nest:fig:voltagetrace}). For this, we use the
\lstinline!pylab! plotting routines of Python's Matplotlib package.
The Python code for this small model is shown below.

\begin{lstlisting}
import nest
import nest.voltage_trace
neuron = nest.Create('iaf_neuron') !\label{nest:ln:Create1}!
sine = nest.Create('ac_generator', 1, !\label{nest:ln:Create2}!
                   {'amplitude': 100.0,
                    'frequency': 2.0})
noise= nest.Create('poisson_generator', 2, !\label{nest:ln:Create3}!
                    [{'rate': 70000.0}, 
                     {'rate': 20000.0}])
voltmeter = nest.Create('voltmeter',1, !\label{nest:ln:Create4}!
                        {'withgid': True})
nest.Connect(sine, neuron)  !\label{nest:ln:Connect1}!
nest.Connect(voltmeter, neuron)
nest.Connect(noise[:1], neuron, syn_spec={'weight': 1.0, 'delay': 1.0}) 
nest.Connect(noise[1:], neuron, syn_spec={'weight': -1.0, 'delay': 1.0})  !\label{nest:ln:Connect2}!
nest.Simulate(1000.0)
nest.voltage_trace.from_device(voltmeter)
\end{lstlisting}
We will now go through the simulation script and explain the
individual steps. The first two lines \lstinline!import! the modules\index{NEST!Python module}
\lstinline!nest!  and its sub-module \lstinline!voltage_trace!. The
\lstinline!nest!  module must be imported in every interactive session
and in every Python script in which you wish to use NEST. NEST is a
C++ library that provides a simulation kernel, many neuron and synapse
models, and the simulation language interpreter SLI. The library which
links the NEST simulation language interpreter to the Python
interpreter is called PyNEST \citep{Eppler09_12}. 

Importing \lstinline!nest! as shown above puts all NEST commands in
the \emph{namespace} \lstinline!nest!\index{NEST!namespace}. Consequently, all commands must
be prefixed with the name of this namespace.
 
In line \ref{nest:ln:Create1}, we use the command \lstinline!Create!
to produce one node of the type \lstinline!iaf_neuron!. As you see in
lines \ref{nest:ln:Create2}, \ref{nest:ln:Create3}, and
\ref{nest:ln:Create4}, \lstinline!Create! is used for all node types.
The first argument, \lstinline!'iaf_neuron'!, is a string, denoting
the type of node that you want to create.
The second parameter of \lstinline!Create! is an integer representing
the number of nodes you want to create. Thus, whether you want one neuron
or 100,000, you only need to call \lstinline!Create! once.
\lstinline!nest.Models()! provides a list of all available node and
connection models.

The third parameter is either a dictionary or a list of dictionaries,
specifying the parameter settings for the created nodes. If only one
dictionary is given, the same parameters are used for all created
nodes. If an array of dictionaries is given, they are used in order
and their number must match the number of created nodes. This variant
of \lstinline!Create! is used in lines \ref{nest:ln:Create2},
\ref{nest:ln:Create3}, and \ref{nest:ln:Create4} to set the parameters
for the Poisson noise generator, the sine generator (for the
alternating current), and the voltmeter. All parameters of a model
that are not set explicitly are initialized with default values. You
can display them with
{\lstset{emph={model_name},emphstyle=\emph}  % set model_name cursive
\lstinline!nest.GetDefaults(model_name)!}.
Note that only the first
parameter of \lstinline!Create! is mandatory. 

\lstinline!Create! returns a list of integers, the  global
identifiers (or GID for short)\index{NEST!global identifier}\index{NEST!GID}
 of each node created. The GIDs are
assigned in the order in which nodes are created. The first node is
assigned GID 1, the second node GID 2, and so on.

In lines \ref{nest:ln:Connect1} to \ref{nest:ln:Connect2}, the nodes
are connected. First we connect the sine generator and the voltmeter
to the neuron. The command  \lstinline!Connect! takes two or more
arguments. The first argument is a list of source nodes. The second
argument is a list of target nodes. \lstinline!Connect! iterates these
two lists and connects the corresponding pairs.

A node appears in the source position of \lstinline!Connect! if it sends events
to the target node. In our example, the sine generator is in the
source position because it injects an alternating current into the
neuron. The voltmeter is in the source position, because it polls the
membrane potential of the neuron. Other devices may be in the target
position, e.g., the spike detector which receives spike events from a
neuron. If in doubt about the order, consult the documentation of the
respective nodes using NEST's help system. For example, to read the
documentation of the voltmeter you can type
\lstinline!nest.help('voltmeter')!\index{NEST!help}.

Next, we use the command \lstinline!Connect! with the
\lstinline!syn_spec! parameter to connect the
two Poisson generators to the neuron. In this example the synapse
specification \lstinline!syn_spec! provides only weight and delay
values, in this case $\pm 1$~pA input current amplitude and $1$~ms
delay. We will see more advanced uses of  \lstinline!syn_spec! below.

After line \ref{nest:ln:Connect2}, the network is ready. The following
line calls the NEST function \lstinline!Simulate! which runs the
network for 1000 milliseconds. The function returns after the
simulation is finished. Then, function \lstinline!voltage_trace! is
called to plot the membrane potential of the neuron. If you are
running the script for the first time, you may have to tell Python to display
the figure by typing \lstinline!pylab.show()!. You should then see
something similar to Fig.~\ref{nest:fig:voltagetrace}.

If you want to inspect how your network looks so far, you can print
it using the command \lstinline!PrintNetwork()!:

\begin{lstlisting}[numbers=none]
>>>nest.PrintNetwork()
+-[0] root dim=[5]
   |
   +-[1] iaf_neuron
   +-[2] ac_generator
   +-[3]...[4] poisson_generator
   +-[5] voltmeter
\end{lstlisting}

If you run the example a second time, NEST will leave the existing
nodes intact and will create a second instance for each node. To start
a new NEST session without leaving Python, you can call
\lstinline!nest.ResetKernel()!. This function will erase the existing
network so that you can start from scratch.

\section{Example 1: A sparsely connected recurrent network}
\label{nest:sec:brunel}
\begin{figure}[!htb]
\centering
\begin{tabular}{ll}
\textbf{a} & \textbf{b}\\
\includegraphics[width=0.4\linewidth]{figures/brunel_detailed_external_single2.eps}&
\includegraphics[width=0.5\linewidth]{figures/brunel_interactive.eps}
\end{tabular}

\caption{\label{nest:fig:brunel2000} Sketch of the network model
  proposed by \citet{Brunel00}. \textbf{a)} The network consists of
  three populations: $N_E$ excitatory neurons (circle labeled E), $N_I$
  inhibitory neurons (circle labeled I), and a
  population of identical, independent Poisson processes (PGs) representing
  activity from outside the network. Arrows represent connections
  between the network nodes. Triangular arrow-heads represent
  excitatory and round arrow-heads represent inhibitory
  connections. The numbers at the start and end of each arrow indicate
  the multiplicity of the connection. See also table
  \ref{nest:tab:Brunel2000}. \textbf{b)} Spiking activity of 50 neurons
  during the first 300 ms of simulated time as a raster plot. Time is
  shown on the x-axis, neuron id on the y-axis. Each dot corresponds
  to a spike of the respective neuron at the given time. The histogram
  below the raster plot shows the population rate of the network.}
\end{figure}

Next we discuss a model of activity dynamics in a local cortical
network proposed by \citet{Brunel00}. We only describe those parts of
the model which are necessary to understand its NEST
implementation. Please refer to the original paper for further details.

The local cortical network consists of two neuron populations: a
population of $N_E$ excitatory neurons and a population of $N_I$
inhibitory neurons. To mimic the cortical ratio of 80\% excitation
and 20\% inhibition, we assume that $N_E=8000$ and $N_I=2000$. Thus,
our local network has a total of 10,000 neurons.

For both the excitatory and the inhibitory population, we use the same
integrate-and-fire neuron model with current-based synapses. Incoming
excitatory and inhibitory spikes displace the membrane potential $V_m$
by $J_{E}$ and $J_I$, respectively. If $V_m$ reaches the threshold
value $V_{\text{th}}$, the membrane potential is reset to $V_{\text{reset}}$,
a spike is sent with delay $D=1.5$ ms to all post-synaptic
neurons, and the neuron remains refractory for $\tau_{\text{rp}}=2.0$ ms.

The neurons are mutually connected with a probability of
10\%. Specifically, each neuron receives input from $C_{E}= 0.1 \cdot
N_{E}$ excitatory and $C_I= 0.1 \cdot N_{I}$ inhibitory neurons (see
Fig.~\ref{nest:fig:brunel2000}a). The inhibitory synaptic weights
$J_I$ are chosen with respect to the excitatory synaptic weights $J_E$
such that
\begin{equation}
J_I = -g \cdot J_E
\end{equation}
with $g=5.0$ in this example.

In addition to the sparse recurrent inputs from within the local
network, each neuron receives excitatory input from a population of
$C_E$ randomly firing neurons, mimicking the input from the rest of
cortex.  The randomly firing population is modeled as $C_E$
independent and identically distributed Poisson processes with rate
$\nu_{\text{ext}}$. Here, we set $\nu_{\text{ext}}$ to twice the rate $\nu_{\text{th}}$
that is needed to drive a neuron to threshold asymptotically. The
details of the model are summarized in tables
\ref{nest:tab:Brunel2000} and \ref{nest:tab:Brunelparams}.

Fig.~\ref{nest:fig:brunel2000}b shows a raster plot of 50
excitatory neurons during the first 300 ms of simulated time. Time is
shown along the x-axis, neuron id along the y-axis. At $t=0$, all
neurons are in the same state $V_m=0$ and hence there is no spiking
activity. The external stimulus rapidly drives the membrane potentials
towards the threshold. Due to the random nature of the external
stimulus, not all the neurons reach the threshold at the same
time. After a few milliseconds, the neurons start to spike irregularly at
roughly 40 $\mathrm{spikes}/s$. In the original paper, this network
state is called the \emph{asynchronous irregular state} \citep{Brunel00}.

\begin{table}[!htp]
\noindent
\caption{\label{nest:tab:Brunel2000} Summary of the network model,
  proposed by \citet{Brunel00}.} 
\begin{tabularx}{0.95\linewidth}{|l|X|}\hline
%
\multicolumn{2}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{A}{Model Summary}}\\\hline
\textbf{Populations} & Three: excitatory, inhibitory, external input \\\hline
\textbf{Topology} & --- \\\hline
\textbf{Connectivity} & Random convergent connections with probability
$P=0.1$ and fixed in-degree of $C_E=P N_E$ and $C_I=P N_I$.
\\\hline
{\textbf{Neuron model}} & Leaky integrate-and-fire, fixed voltage
threshold, fixed absolute refractory time (voltage clamp) \\\hline
\textbf{Channel models} & --- \\\hline
\textbf{Synapse model} & $\delta$-current inputs (discontinuous
  voltage jumps) \\\hline
\textbf{Plasticity} & ---\\\hline
\textbf{Input} & Independent fixed-rate Poisson spike trains to all
neurons \\\hline
\textbf{Measurements} & Spike activity \\\hline
\end{tabularx}

\vspace{2ex}

\noindent\begin{tabularx}{0.95\linewidth}{|l|l|X|}\hline
\multicolumn{3}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{B}{Populations}}\\\hline
  \textbf{Name} & \textbf{Elements} & \textbf{Size} \\\hline
E & Iaf neuron & $N_{\text{E}} = 4N_{\text{I}}$  \\\hline
I & Iaf neuron & $N_{\text{I}}$ \\\hline
E$_{\text{ext}}$ & Poisson generator & $C_E(N_{\text{E}}+N_{\text{I}})$ \\\hline
\end{tabularx}

\vspace{2ex}

\noindent\begin{tabularx}{0.95\linewidth}{|l|l|l|X|}\hline
\multicolumn{4}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{C}{Connectivity}}\\\hline
\textbf{Name} & \textbf{Source} & \textbf{Target} & \textbf{Pattern} \\\hline
  EE & E & E & 
  Random convergent $C_{\text{E}}\rightarrow 1$, weight $J$, delay $D$ \\\hline
  IE & E & I & 
  Random convergent $C_{\text{E}}\rightarrow 1$, weight $J$, delay $D$ \\\hline
  EI & I & E & 
  Random convergent $C_{\text{I}}\rightarrow 1$, weight $-gJ$, delay $D$ \\\hline
  II & I & I & 
  Random convergent $C_{\text{I}}\rightarrow 1$, weight $-gJ$, delay $D$ \\\hline
  Ext& E$_{\text{ext}}$ & E $\cup$ I & 
  Non-overlapping $C_{\text{E}}\rightarrow 1$, weight $J$, delay $D$ \\\hline
\end{tabularx}

\vspace{2ex}

\noindent\begin{tabularx}{0.95\linewidth}{|l|X|}\hline
\multicolumn{2}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{D}{Neuron and Synapse Model}}\\\hline
\textbf{Name} & Iaf neuron \\\hline
\textbf{Type} & Leaky integrate-and-fire, $\delta$-current input\\\hline
\raisebox{-4.5ex}{\parbox{5em}{\textbf{Sub\-threshold dynamics}}} &
\rule{1em}{0em}\vspace*{-3.5ex}
    \begin{equation*}
      \begin{array}{r@{\;=\;}ll}
      \tau_m \dot{V_m}(t) & -V_m(t) + R_m I(t) &\text{if not refractory}\; (t > t^*+\tau_{\text{rp}}) \\[1ex]
      V_m(t) & V_{\text{r}} & \text{while refractory}\; (t^*<t\leq t^*+\tau_{\text{rp}}) \\[2ex]
      I(t) & \multicolumn{2}{l}{\frac{\tau_m}{R_m} \sum_{\tilde{t}} w
        \delta(t-(\tilde{t}+D))}
      \end{array}
    \end{equation*} 
\vspace*{-2.5ex}\rule{1em}{0em}
 \\\hline
\multirow{3}{*}{\textbf{Spiking}} & 
   If $V_m(t-)<V_{\theta} \wedge V_m(t+)\geq V_{\theta}$
\vspace*{-1ex}
\begin{enumerate}\setlength{\itemsep}{-0.5ex}
\item set $t^* = t$
\item emit spike with time-stamp $t^*$
\end{enumerate}
\vspace*{-4ex}\rule{1em}{0em}
\\\hline
\end{tabularx}

\vspace{2ex}

\noindent\begin{tabularx}{0.95\linewidth}{|l|X|}\hline
\multicolumn{2}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{E}{Input}}\\\hline
\textbf{Type} & \textbf{Description} \\\hline
{Poisson generators} & Fixed rate $\nu_{\text{ext}}$, $C_{\text{E}}$
generators per neuron, each generator projects to one neuron\\\hline
\end{tabularx}

\vspace{2ex}

\noindent\begin{tabularx}{0.95\linewidth}{|X|}\hline
  \multicolumn{1}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{F}{Measurements}}\\\hline
  Spike activity as raster plots, rates and ``global frequencies'', no
  details given \\\hline
\end{tabularx}
\end{table}
\begin{table}[!htp]
\noindent
\caption{\label{nest:tab:Brunelparams} Summary of the network
  parameters for the model, proposed by \citet{Brunel00}.} 
\begin{tabularx}{0.95\linewidth}{Xr}
%
\multicolumn{2}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{G}{Network
  Parameters}}\\
 \textbf{Parameter} & \textbf{Value}\\\hline
 Number of excitatory neurons $N_E$ & 8000 \\
 Number of inhibitory neurons $N_I$ & 2000\\
 Excitatory synapses per neuron $C_E$ & 800 \\
 Inhibitory synapses per neuron $C_E$ & 200 \\
\hline
\end{tabularx}
\begin{tabularx}{0.95\linewidth}{Xr}
\multicolumn{2}{|l|}{\color{white}\cellcolor[gray]{0.0}\hdr{H}{Neuron
  Parameters}}\\
 \textbf{Parameter} & \textbf{Value}\\\hline
Membrane time constant $\tau_m$ & 20 ms\\
Refractory period $\tau_{\text{rp}}$ & 2 ms\\
Firing threshold $V_{\text{th}}$ & 20 mV\\
Membrane capatitance $C_m$ & 1pF\\
Resting potential    $V_E$ & 0 mV\\
Reset potential      $V_{\text{reset}}$ & 10 mV\\
Excitatory PSP amplitude $J_E$ & 0.1 mV\\
Inhibitory PSP amplitude $J_I$ & -0.5 mV\\
Synaptic delay $D$ & 1.5 ms \\
Background rate $\eta$ & 2.0 \\
\hline
\end{tabularx}
\end{table}

\subsection{NEST Implementation}\label{nest:sec:brunel_impl}

We now show how this model is implemented in NEST. Along the way, we
explain the required steps and NEST commands in more detail so that
you can apply them to your own models.

\subsubsection{Preparations}
The first three lines import NEST, a NEST module for raster plots, and
the plotting package \lstinline!pylab!. We then assign the various model
parameters to variables.
\begin{lstlisting}[name=Brunel_interactive]
import nest
import nest.raster_plot
import pylab
g       = 5.0 
eta     = 2.0 !\label{nest:ln:eta}!
delay   = 1.5 
tau_m   = 20.0
V_th    = 20.0
N_E = 8000
N_I = 2000
N_neurons = N_E+N_I
C_E    = N_E/10
C_I    = N_I/10
J_E  = 0.1
J_I  = -g*J_E
nu_ex  = eta*V_th/(J_E*C_E*tau_m) !\label{nest:ln:nu_ex}!
p_rate = 1000.0*nu_ex*C_E          !\label{nest:ln:p_rate}!
\end{lstlisting}
In line \ref{nest:ln:nu_ex}, we compute the firing rate
\lstinline!nu_ex! ($\nu_{\text{ext}}$) of a neuron in the external population. We define
\lstinline!nu_ex! as the product of a constant \lstinline!eta! times
the threshold rate $\nu_{\text{th}}$, i.e. the steady state firing
rate which is needed to bring a neuron to threshold. The value of the
scaling constant  \lstinline!eta! is defined in line \ref{nest:ln:eta}.

In line \ref{nest:ln:p_rate}, we compute the population rate of the
whole external population. With $C_E$ neurons, the population rate is
simply the product \lstinline!nu_ex*C_E!. The factor 1000.0 in the
product changes the units from spikes per ms to spikes per second.
 
\begin{lstlisting}[name=Brunel_interactive]
nest.SetKernelStatus({'print_time': True}) !\label{nest:ln:kernelstatus}!
\end{lstlisting}
Next, we prepare the simulation kernel of NEST (line
\ref{nest:ln:kernelstatus}). The command \lstinline!SetKernelStatus!
modifies parameters of the simulation kernel. The argument is a Python
dictionary with \emph{key}:\emph{value} pairs. Here, we set the NEST
kernel to print the progress of the simulation time during simulation.

\subsubsection{Creating neurons and devices}

As a rule of thumb, we recommend that you create all elements in your
network, i.e., neurons, stimulating devices and recording devices
first, before creating any connections. 

\begin{lstlisting}[name=Brunel_interactive]
nest.SetDefaults('iaf_psc_delta',  !\label{nest:ln:modeldefaults}!
                 {'C_m': 1.0,
                  'tau_m': tau_m,
                  't_ref': 2.0,
                  'E_L': 0.0,
                  'V_th': V_th,
                  'V_reset': 10.0})!\label{nest:ln:modeldefaults-e}!
\end{lstlisting}

In lines \ref{nest:ln:modeldefaults} to \ref{nest:ln:modeldefaults-e},
we change the parameters of the neuron model we want to use from the
built-in values to the defaults for our investigation.
\lstinline!SetDefaults! expects two parameters. The first is a string,
naming the model for which the default parameters should be
changed. Our neuron model for this simulation is the simplest
integrate-and-fire model in NEST's repertoire:
\lstinline!'iaf_psc_delta'!. The second parameter is a dictionary with
parameters and their new values, entries separated by commas. All
parameter values are taken from Brunel's paper \citep{Brunel00} and we
insert them directly for brevity. Only the membrane time constant
\lstinline!tau_m! and the threshold potential \lstinline!V_th! are
read from variables, because these values are needed in several places.

\begin{lstlisting}[name=Brunel_interactive]
nodes   = nest.Create('iaf_psc_delta', N_neurons) !\label{nest:ln:create1}!
nodes_E = nodes[:N_E] !\label{nest:ln:nodes_e}!
nodes_I = nodes[N_E:] !\label{nest:ln:nodes_i}!

noise = nest.Create('poisson_generator',1,{'rate': p_rate})!\label{nest:ln:create2}!

spikes = nest.Create('spike_detector',2,    !\label{nest:ln:spikedetect}!
                     [{'label': 'brunel-py-ex'},
                      {'label': 'brunel-py-in'}])
spikes_E = spikes[:1]         !\label{nest:ln:spikes_e}!             
spikes_I = spikes[1:]
\end{lstlisting} 

In line \ref{nest:ln:create1} we create the neurons. 
\lstinline!Create! returns a list of the global IDs which
are consecutive numbers from 1 to \lstinline!N_neurons!. 
We split this range into excitatory and inhibitory neurons. In
line \ref{nest:ln:nodes_e} we select the first \lstinline!N_E!
elements from the list \lstinline!nodes! and assign them to the
variable \lstinline!nodes_E!. This list now holds the GIDs of the
excitatory neurons. 

Similarly, in line \ref{nest:ln:nodes_i} we assign the range from position
\lstinline!N_E! to the end of the list to the variable
\lstinline!nodes_I!. This list now holds the GIDs of all inhibitory
neurons. The selection is carried out using standard Python list commands. You
may want to consult the Python documentation for more details.

Next, we create and connect the external population and some devices
to measure the spiking activity in the network.

In line \ref{nest:ln:create2}, we create a device known as a
\lstinline!poisson_generator!, which produces a spike train governed
by a Poisson process at a given rate. We use the third parameter of
\lstinline!Create! to initialize the rate of the Poisson process to
the population rate \lstinline!p_rate!  which we previously computed
in line \ref{nest:ln:p_rate}.

If a Poisson generator is connected to $n$ targets, it generates $n$
independent and identically distributed spike trains. Thus, we only
need one generator to model an entire population of randomly firing
neurons.

 To observe how the neurons in the
recurrent network respond to the random spikes from the external
population, we create two spike detectors in line
\ref{nest:ln:spikedetect}; one for the excitatory neurons and one for the
inhibitory neurons. By default, each detector writes its spikes into a
file whose name is automatically generated from the device type and
its global id. We use
the third argument of \lstinline!Create! to give each spike detector a
\lstinline!'label'!, which will be part of the name of the output file written
by the detector. Since two devices are created, we supply a list
of dictionaries.

In line \ref{nest:ln:spikes_e}, we store the GID of the first spike
detector in a one-element list and assign it to the variable
\lstinline!spikes_E!. In the next line, we do the same for the second
spike detector that is dedicated to the inhibitory population.

\subsubsection{Connecting the network}

Once all network elements are in place, we connect them.

\begin{lstlisting}[name=Brunel_interactive] 
nest.CopyModel('static_synapse_hom_w',  !\label{nest:ln:copymodel}!
               'excitatory',
               {'weight':J_E, 
                'delay':delay})
nest.Connect(nodes_E, nodes,            !\label{nest:ln:randomconnect}!
             {'rule': 'fixed_indegree', 
              'indegree': C_E},
             'excitatory')
nest.CopyModel('static_synapse_hom_w', !\label{nest:ln:copymodel1}!
               'inhibitory',
               {'weight':J_I, 
                'delay':delay})
nest.Connect(nodes_I, nodes,
             {'rule': 'fixed_indegree', 
              'indegree': C_I},
             'inhibitory')  !\label{nest:ln:randomconnect1}!
\end{lstlisting} 

On line \ref{nest:ln:copymodel}, we create a new connection
type \lstinline!'excitatory'! by copying the built-in connection type
\lstinline!'static_synapse_hom_w'! while changing its default values
for \emph{weight} and \emph{delay}. The command \lstinline!CopyModel!
expects either two or three arguments: the name of an existing neuron
or synapse model, the name of the new model, and optionally a
dictionary with the new default values of the new model.

The connection type \lstinline!'static_synapse_hom_w'! uses the same
values of weight for all synapses. This saves memory for
networks in which these values are identical for all connections. In
Section \ref{nest:sec:random} we use a different connection model to
implement randomized weights and delays.

Having created and parameterized an appropriate synapse model, we draw
the incoming excitatory connections for each neuron (line
\ref{nest:ln:randomconnect}). The function
\lstinline!Connect!  expects four arguments: a list of
source nodes, a list of target nodes, a connection rule, and a synapse
specification. Some connection rules, in particular
\lstinline!'one_to_one'! and \lstinline!'all_to_all'! require no
parameters and can be specified as strings. All other connection rules
must be specfied as a dictionary, while at least must contain the key
\lstinline!'rule'! specifying a connection rule;
\lstinline!nest.ConnectionRules()! shows all connection rules. The
remaining dictionary entries depend on the particular rule. We use the
\lstinline!'fixed_indegree'! rule, which creates exactly
\lstinline!indegree! connections to each target neurons; in previous
versions of NEST, this connectivity was provided by
\lstinline!RandomConvergentConnect!. 

The final argument specifies the synapse model to be used, here the
\lstinline!'excitatory'! model we defined previously.


In lines \ref{nest:ln:copymodel1} to \ref{nest:ln:randomconnect1} we
repeat the same steps for the inhibitory connections: we create a new
connection type and draw the incoming inhibitory connections for all neurons.

\begin{lstlisting}[name=Brunel_interactive]
nest.Connect(noise, nodes, syn_spec='excitatory') !\label{nest:ln:divergent}!

N_rec   = 50 !\label{nest:ln:N_rec}!
nest.Connect(nodes_E[:N_rec], spikes_E)  !\label{nest:ln:convergent}!
nest.Connect(nodes_I[:N_rec], spikes_I)
\end{lstlisting}

In the next line (\ref{nest:ln:divergent}), we use
\lstinline!Connect!\index{NEST!divergent connection} to
connect the Poisson generator to all nodes of the local network. Since
these connections are excitatory, we use the \lstinline!'excitatory'!
connection type. Finally, we connect a subset of excitatory and
inhibitory neurons to the spike detectors to record from them. If no connection rule
is given, \lstinline!Connect! connects all sources to all targets (\lstinline!all_to_all! rule), 
i.e., on line~\ref{nest:ln:divergent} the noise generator is connected to all neurons 
(previously \lstinline!DivergentConnect!), while on line~\ref{nest:ln:convergent}, all recorded 
excitatory neurons are connected to the \lstinline!spikes_E! spike detector 
(previously \lstinline!ConvergentConnect!).

Our network consists of 10,000 neurons, all of which having the same
activity statistics due to the random connectivity. Thus, it suffices
to record from a representative sample of neurons, rather than from
the entire network. Here, we choose to record from 50 neurons and
assign this number to the variable \lstinline!N_rec!. We then connect
the first 50 excitatory neurons to their spike detector. Again, we use
standard Python list operations to select \lstinline!N_rec! neurons
from the list of all excitatory nodes. Alternatively, we could select
50 neurons at random, but since the neuron order has no meaning in
this model, the two approaches would yield qualitatively the same
results. Finally, we repeat this step for the inhibitory neurons.

\subsubsection{Simulating the network}

Now everything is set up and we can run the simulation.

\begin{lstlisting}[name=Brunel_interactive]
simtime=300   !\label{nest:ln:simtime}!
nest.Simulate(simtime)
events = nest.GetStatus(spikes,'n_events')!\label{nest:ln:events}!
rate_ex= events[0]/simtime*1000.0/N_rec   !\label{nest:ln:rate_ex}!
print "Excitatory rate   : %.2f 1/s" % rate_ex !\label{nest:ln:pr_rate_ex}!
rate_in= events[1]/simtime*1000.0/N_rec   !\label{nest:ln:rate_in}!
print "Inhibitory rate   : %.2f 1/s" % rate_in !\label{nest:ln:pr_rate_in}!
nest.raster_plot.from_device(spikes_E, hist=True)  !\label{nest:ln:raster}!
\end{lstlisting}
In line \ref{nest:ln:simtime}, we select a simulation time of
300 milliseconds and assign it to a variable. Next, we call the NEST
command \lstinline!Simulate! to run the simulation for 300 ms. During
simulation, the Poisson generators send spikes into the network
and cause the neurons to fire. The spike detectors receive spikes
from the neurons and write them to a file, or to memory.

When the function returns, the simulation time has progressed by
300 ms. You can call \lstinline!Simulate! as often as you like and
with different arguments. NEST will resume the simulation at the point
where it was last stopped. Thus, you can partition your simulation time
into small epochs to regularly inspect the progress of your model.

After the simulation is finished, we compute the firing rate of the
excitatory neurons (line \ref{nest:ln:rate_ex}) and the inhibitory
neurons (line \ref{nest:ln:rate_in}). Finally, we call the NEST
function \lstinline!raster_plot! to produce the raster plot shown in
Fig.~\ref{nest:fig:brunel2000}b. \lstinline!raster_plot! has two
modes. \lstinline!raster_plot.from_device! expects the global ID of a
spike detector.  \lstinline!raster_plot.from_file! expects the name of
a data-file. This is useful to plot data without repeating a
simulation.


\section{Parallel simulation}\label{nest:sec:parallel}

Large network models often require too much time or computer memory to
be conveniently simulated on a single computer. For example, if we increase the number of
neurons in the previous model to 100,000, there will be a total of
$10^9$ connections, which won't fit into the memory of most computers.
Similarly, if we use plastic synapses (see Section
\ref{nest:sec:plastic}) and run the model for minutes or hours of
simulated time, the execution times become uncomfortably long.
 
To address this issue, NEST has two modes of parallelization:
multi-threading and distribution. Multi-threaded and distributed
simulation can be used in isolation or in combination
\citep{Ples:2007(672)}, and both modes allow you to connect and run
networks more quickly than in the serial case.  

Multi-threading\index{NEST!multi-threading} means that NEST uses all
available processors or cores of the computer. Today, most desktop
computers and even laptops have at least two processing cores. Thus,
you can use NEST's multi-threaded mode to make your simulations
execute more quickly whilst still maintaining the convenience of
interactive sessions. Since a given computer has a fixed memory size,
multi-threaded simulation can only reduce execution times. It cannot
solve the problem that large models exhaust the computer's memory.

Distribution\index{NEST!distributed simulation} means that NEST uses
many computers in a network or computer cluster. Since each computer
contributes memory, distributed simulation allows you to simulate
models that are too large for a single computer. However, in
distributed mode it is not currently possible to use NEST
interactively.

In most cases, writing a simulation script to be run in parallel is as
easy as writing one to be executed on a single processor. Only minimal
changes are required, as described below, and you can ignore the fact
that the simulation is actually executed by more than one core or
computer. However, in some cases your knowledge about the distributed
nature of the simulation can help you improve efficiency even
further. For example, in the distributed mode, all computers execute
the same simulation script. We can improve performance if the
script running on a specific computer only tries to execute commands
relating to nodes that are represented on the
same computer. An example of this technique is shown below in Section
\ref{nest:sec:rand_example}.

To switch NEST into
multi-threaded mode, you only have to add one line to your simulation
script:
\begin{lstlisting}[numbers=none]
nest.SetKernelStatus({'local_num_threads': n}) !\label{nest:ln:loc_num_threads}!
\end{lstlisting}
Here, \lstinline!n! is the number of threads you want to use. It is
important that you set the number of threads \emph{before} you create
any nodes. If you try to change the number of threads after nodes were
created, NEST will issue an error.

A good choice for the number of threads is the number of cores or
processors on your computer. If your processor supports
hyperthreading, you can select an even higher number of threads.

The distributed mode of NEST is particularly useful for large simulations
for which not only the processing speed, but also the memory of a single
computer are insufficient.  The distributed mode of NEST uses the
Message Passing Interface (MPI, \citet{MPI2009}), a library that must be
installed on your computer network when you install NEST. For details,
please refer to NEST's website at \url{www.nest-initiative.org}.

The distributed mode of NEST is also easy to use. All you need to do
is start NEST with the MPI command \lstinline!mpirun!:
\begin{lstlisting}[numbers=none]
mpirun -np m python script.py
\end{lstlisting}
where \lstinline!m! is the number of MPI processes that should be
started. One sensible choice for \lstinline!m! is the total number of
cores available on the cluster. Another reasonable choice is the
number of physically distinct machines, utilizing their cores through
multithreading as described above. This can be useful on clusters of
multi-core computers. 

In NEST, processes and threads are both mapped to \emph{virtual
  processes}\index{NEST!virtual process} \citep{Ples:2007(672)}. If a
simulation is started with \lstinline!m! MPI processes and
\lstinline!n!  threads on each process, then there are
\lstinline!m!$\times$\lstinline!n!  virtual processes. You can obtain
the number of virtual processes in a simulation with
\begin{lstlisting}[numbers=none]
nest.GetKernelStatus('total_num_virtual_procs')
\end{lstlisting}

The virtual process concept is reflected in the labeling of output
files. For example, the data files for the excitatory spikes produced
by the network discussed here follow the form
\lstinline!brunel-py-ex-x-y.gdf!, where \lstinline!x! is the id of the
data recording node and \lstinline!y! is the id of the virtual
process.

\section{Randomness in NEST}\label{nest:sec:random}

NEST has built-in random number sources that can be used for tasks
such as randomizing spike trains or network connectivity. In this
section, we discuss some of the issues related to the use of random
numbers in parallel simulations. In
Section~\ref{nest:sec:rand_example}, we illustrate how to randomize
parameters in a network.

Let us first consider the case that a simulation script does not
explicitly generate random numbers. In this case, NEST produces
identical simulation results for a given number of virtual processes,
irrespective of how the virtual processes are partitioned into threads
and MPI processes. The only difference between the output of two
simulations with different configurations of threads and processes
resulting in the same number of virtual processes is the result of
query commands such as \lstinline!GetStatus!. These commands 
gather data over threads on the local machine, but not over remote
machines.

In the case that random numbers are explictly generated in the
simulation script, more care must be taken to produce results that are
independent of the parallel configuration. Consider, for example, a
simulation where two threads have to draw a random number from a
single random number generator. Since only one thread can access the
random number generator at a time, the outcome of the simulation will
depend on the access order.

Ideally, all random numbers in a simulation should come from a single
source. In a serial simulation this is trivial to implement, but in
parallel simulations this would require shipping a large number of
random numbers from a central random number generator (RNG) to all
processes. This is impractical.  Therefore, NEST uses one independent
random number generator\index{NEST!random number generator} on each
virtual process. Not all random number generators can be used in
parallel simulations, because many cannot reliably produce
uncorrelated parallel streams. Fortunately, recent years have seen
great progress in RNG research and there is a range of random number
generators that can be used with great fidelity in parallel
applications.

Based on this knowledge, each virtual process (VP) in NEST has its own
RNG. Numbers from these RNGs are used to
\begin{itemize}
\item choose random convergent connections
\item create random spike trains (e.g.\ \lstinline!poisson_generator!)
  or random currents
  (e.g.\ \lstinline!noise_generator!).
\end{itemize}

In order to randomize model parameters in a PyNEST script, it is
convenient to use the random number generators provided by
NumPy. To ensure consistent results for a given number
of virtual processes, each virtual process should use a separate
Python RNG. Thus, in a simulation running on $\Nvp$ virtual processes,
there should be $2\Nvp+1$ RNGs in total:
\begin{itemize}
\item the global NEST RNG;
\item one RNG per VP in NEST;
\item one RNG per VP in Python.
\end{itemize}

We need to provide separate seed values for each of these generators.
Modern random number generators work equally well for all seed
values. We thus suggest the following approach to choosing seeds: For
each simulation run, choose a {master seed} $msd$ and seed the RNGs
with seeds $msd$, $msd+1$, \dots $msd+2\Nvp$. Any two master seeds must
differ by at least $2\Nvp+1$ to avoid correlations between simulations.

By default, NEST uses Knuth's lagged Fibonacci RNG, which has the nice
property that each seed value provides a different sequence of
some $2^{70}$ random numbers \citep[Ch.~3.6]{Knut:Art(2)(1998)}. Python
uses the Mersenne Twister MT19937 generator \citep{Mats:1998(3)},
which provides no explicit guarantees, but given the enormous state
space of this generator it appears astronomically unlikely that
neighboring integer seeds would yield overlapping number
sequences. For a recent overview of RNGs, see
\cite{Lecu:2007(22)}. For general introductions to random number
generation, see \citet{Gent:Rand(2003)},
\citet[Ch.~3]{Knut:Art(2)(1998)}, or \citet{Ples:2010(399)}.

\section{Example 2: Randomizing neurons and synapses}\label{nest:sec:rand_example}

\renewcommand*\thelstnumber{r\arabic{lstnumber}}

Let us now consider how to randomize some neuron and synapse
parameters in the sparsely connected network model introduced in
Section~\ref{nest:sec:brunel}. We shall 
\begin{itemize}
\item explicitly seed the random number generators;
\item randomize the initial membrane potential of all neurons;
\item randomize the weights of the recurrent excitatory connections.
\end{itemize}
We discuss here only those parts of the model script that differ from
the script discussed in Section~\ref{nest:sec:brunel_impl}; the complete
script \lstinline!brunel2000-rand.py! is part of the NEST examples. 

We begin by importing the NumPy package to get access to its random
generator functions:
\begin{lstlisting}[numbers=none]
import numpy
\end{lstlisting}
After line~\ref{nest:ln:loc_num_threads} of the original script (cf.\
p.~\pageref{nest:ln:loc_num_threads}), we insert code to seed the
random number generators: 
\begin{lstlisting}[name=brunel-rand]
msd = 1000   # master seed
msdrange1 = range(msd, msd+n_vp)
n_vp = nest.GetKernelStatus('total_num_virtual_procs') !\label{nest:ln:getnvp}!
pyrngs = [numpy.random.RandomState(s) for s in msdrange1] !\label{nest:ln:pyrng}!
msdrange2 = range(msd+n_vp+1, msd+1+2*n_vp)
nest.SetKernelStatus({'grng_seed': msd+n_vp,  !\label{nest:ln:grng}!
                      'rng_seeds': msdrange2}) !\label{nest:ln:rngs}!
\end{lstlisting}
We first define the master seed\index{NEST!master
  seed}\index{NEST!seed} \lstinline!msd! and then obtain the number of
virtual processes \lstinline!n_vp!.  On line~\ref{nest:ln:pyrng} we
then create a list of \lstinline!n_vp!  NumPy random number generators
with seeds \lstinline!msd!, \lstinline!msd+1!, \dots
\lstinline!msd+n_vp-1!. The next two lines set new seeds for the
built-in NEST RNGs: the global RNG is seeded with
\lstinline!msd+n_vp!, the per-virtual-process RNGs with
\lstinline!msd+n_vp+1!, \dots, \lstinline!msd+2*n_vp!. Note that the
seeds for the per-virtual-process RNGs must always be passed as a
list, even in a serial simulation.

After creating the neurons as before, we insert the following code
after line~\ref{nest:ln:nodes_i} to randomize the membrane potential
of all neurons:
\begin{lstlisting}[name=brunel-rand]
node_info   = nest.GetStatus(nodes)   !\label{nest:ln:nodeinfo}!
local_nodes = [(ni['global_id'], ni['vp'])   !\label{nest:ln:localnodes}!
               for ni in node_info if ni['local']]
for gid,vp in local_nodes:   !\label{nest:ln:randvmfor}!
  nest.SetStatus([gid], {'V_m': pyrngs[vp].uniform(-V_th,V_th)}) !\label{nest:ln:randvm}!
\end{lstlisting}
In this code, we meet the concept of \emph{local} nodes for the first
time \citep{Ples:2007(672)}. In serial and multi-threaded
simulations, all nodes are local. In an MPI-based simulation with $m$
MPI processes, each MPI process represents and is responsible for
updating (approximately) $1/m$-th of all nodes---these are the local
nodes for each process. In line \ref{nest:ln:nodeinfo} we obtain status 
information for each node; for local nodes, this will be full information, 
for non-local nodes this will only be minimal information. We then use a list
 comprehension to create
a list of \lstinline!gid! and \lstinline!vp! tuples for all local
nodes. The \lstinline!for!-loop then iterates over this list and draws
for each node a membrane potential value uniformly distributed in
$[-V_{\text{th}}, V_{\text{th}})$, i.e., $[-20\text{mV},
20\text{mV})$. We draw the inital membrane potential for each node
from the NumPy RNG assigned to the virtual process \lstinline!vp!
responsible for updating that node.

As the next step, we create excitatory recurrent connections with the
same connection rule as in the original script, but with randomized
weights. To this end, we replace the code on lines
\ref{nest:ln:copymodel}--\ref{nest:ln:randomconnect} of the original
script with
\begin{lstlisting}[name=brunel-rand]
nest.CopyModel('static_synapse', 'excitatory')
nest.Connect(nodes_E, nodes,
             {'rule': 'fixed_indegree', 
              'indegree': C_E},
             {'model': 'excitatory',  !\label{nest:w_rand_beg}!
              'delay': delay,
              'weight': {'distribution': 'uniform',
                         'low': 0.5 * J_E, 
                         'high': 1.5 * J_E}}) !\label{nest:w_rand_end}!
\end{lstlisting}
The first difference to the original is that we base the excitatory
synapse model on the built-in \lstinline!static_synapse! model instead
of \lstinline!static_synapse_hom_w!, as the latter implies equal
weights for all synapses. The second difference is that we randomize
the initial weights. To this end, we have replaced the simple synapse
specification \lstinline!'excitatory'! with a synapse specification
dictionary on lines \ref{nest:w_rand_beg}--\ref{nest:w_rand_end}. Such
a dictionary must always contain the key \lstinline!'model'! providing
the synapse model to use. In addition, we specify a fixed delay, and a
distribution from which to draw the weights here a uniform
distribution over $[J_E/2, 3J_E/2)$. NEST will automatically use the
correct random number generator for each weight. 

To see all available random distributions, please run
\lstinline!nest.sli_run('rdevdict info')!. To access documentation for
an individual distribution, run, e.g., \lstinline!nest.help('rdevdict::binomial')!.
These distributions can be
used for all parameters of a synapse.


Before starting our simulation, we want to visualize the randomized
initial membrane potentials and weights. To this end, we insert the
following code just before we start the simulation:
\begin{lstlisting}[name=brunel-rand]
pylab.figure()
V_E = nest.GetStatus(nodes_E[:N_rec], 'V_m')  !\label{nest:ln:getvm}!
pylab.hist(V_E, bins=10)
pylab.figure()
ex_conns = nest.GetConnections(nodes_E[:N_rec],!\label{nest:ln:findconns}!
                                synapse_model='excitatory')
w = nest.GetStatus(ex_conns, 'weight')  !\label{nest:ln:getw}!
pylab.hist(w, bins=100)
\end{lstlisting}
Line~\ref{nest:ln:getvm} retrieves the membrane potentials of all 50
recorded neurons. The data is then displayed as a histogram with 10
bins, see Fig.~\ref{nest:fig:rand_results}. 
Line~\ref{nest:ln:findconns} finds 
all connections that
\begin{itemize}
\item have one of the 50 recorded excitatory neurons as source;
\item have any local node as target;
\item and are of type \lstinline!excitatory!.
\end{itemize} 
In line~\ref{nest:ln:getw}, we then use \lstinline!GetStatus()! to
obtain the weights of these connections. Running the script in a
single MPI process, we record approximately 50,000 weights, which we
display in a histogram with 100 bins as shown in
Fig.~\ref{nest:fig:rand_results}.

Note that the code on lines \ref{nest:ln:getvm}--\ref{nest:ln:getw}
will return complete results only when run in a single MPI
process. Otherwise, only data from local neurons or connections with
local targets will be obtained. It is currently not possible to
collect recorded data across MPI processes in NEST. In distributed
simulations, you should thus let recording devices write data to files
and collect the data after the simulation is complete.

\begin{figure}[tb]
\centering
\begin{tabular}{lll}
\textbf{a}  & \textbf{b} & \textbf{c}\\ 
\includegraphics[width=0.3\linewidth]{figures/rand_Vm.eps} &
\includegraphics[width=0.3\linewidth]{figures/rand_w.eps}  &
\includegraphics[width=0.3\linewidth]{figures/rand_raster.eps}
\end{tabular}
\caption{\label{nest:fig:rand_results} \textbf{a)} Distribution of membrane
  potentials $V_m$ of 50 excitatory neurons after random
  initialization. \textbf{b)} Distribution of weights of randomized
  weights of approximately 50,000 recurrent connections originating
  from 50 excitatory neurons. \textbf{c)} Spiking activity of 50
  excitatory neurons during the first 300 ms of network simulation;
  compare with the corresponding diagram for the same network without
  randomization of initial membrane potentials and weights in
  Fig.~\ref{nest:fig:brunel2000}.}
\end{figure}


The result of the simulation is displayed as before. Comparing the
raster plot from the simulation with randomized initial membrane
potentials in Fig.~\ref{nest:fig:rand_results} with the same plot for the
original network in Fig.~\ref{nest:fig:brunel2000} reveals that the
membrane potential randomization has prevented the synchronous onset
of activity in the network.

As a final point, we make a slight improvement to the rate computation on
lines \ref{nest:ln:rate_ex}--\ref{nest:ln:pr_rate_in} of the original
script. Spike detectors count only spikes from neurons on the local
MPI process. Thus, the original computation is correct only for a
single MPI process. To obtain meaningful results when simulating
on several MPI processes, we count how many of the \lstinline!N_rec!
recorded nodes are local and use that number to compute the rates:
\begin{lstlisting}[name=brunel-rand]
N_rec_local_E = sum(nest.GetStatus(nodes_E[:N_rec], 'local'))
rate_ex= events[0]/simtime*1000.0/N_rec_local_E
\end{lstlisting}
Each MPI process then reports the rate of activity of its locally
recorded nodes.



\section{Example 3: Plastic Networks}\label{nest:sec:plastic}

\renewcommand*\thelstnumber{p\arabic{lstnumber}}

NEST provides synapse models with a variety of short-term and
long-term dynamics. To illustrate this, we extend the sparsely
connected network introduced in section \ref{nest:sec:brunel} with
randomized synaptic weights as described in section
\ref{nest:sec:random} to incorporate spike-timing dependent plasticity
\citep{Bi98} at its recurrent excitatory-excitatory synapses.

We create all nodes and randomize their initial membrane potentials
as before.
%
\begin{figure}[!tbp]
\centering
\includegraphics[width=0.7\linewidth]{figures/rand_plas_w.eps}
\caption{\label{nest:fig:plasticweights} Distribution of synaptic
  weights in the plastic network simulation after 300 ms.} 
\end{figure}
%
We then generate a plastic synapse model for the excitatory-excitatory
connections and a static synapse model for the excitatory-inhibitory
connections:
\begin{lstlisting}[name=brunel-plastic]
nest.CopyModel('stdp_synapse_hom',
               'excitatory-plastic',
               {'alpha':STDP_alpha,
                'Wmax':STDP_Wmax})
nest.CopyModel('static_synapse', 'excitatory-static')
\end{lstlisting}
Here, we set the parameters \lstinline!alpha! and \lstinline!Wmax!  of
the synapse model but use the default settings for all its other
parameters. The \lstinline!_hom! suffix in the synapse model name
indicates that all plasticity parameters such as \lstinline!alpha! and
\lstinline!Wmax! are shared by all synapses of this model.

We again use \lstinline!nest.Connect! to create connections with
randomized weights:
\begin{lstlisting}[name=brunel-plastic]
nest.Connect(nodes_E, nodes_E,
             {'rule': 'fixed_indegree', 
              'indegree': C_E},
             {'model': 'excitatory_plastic', 
              'delay': delay,
              'weight': {'distribution': 'uniform',
                         'low': 0.5 * J_E, 
                         'high': 1.5 * J_E}})

nest.Connect(nodes_E, nodes_I,
             {'rule': 'fixed_indegree', 
              'indegree': C_E},
             {'model': 'excitatory_static', 
              'delay': delay,
              'weight': {'distribution': 'uniform',
                         'low': 0.5 * J_E, 
                         'high': 1.5 * J_E}})
\end{lstlisting}
After a period of simulation, we can access the plastic synaptic
weights for analysis:
\begin{lstlisting}
w = nest.GetStatus(nest.GetConnections(nodes_E[:N_rec],
                        synapse_model='excitatory-plastic'),
                        'weight')
\end{lstlisting}

Plotting a histogram of the synaptic weights reveals that the initial
uniform distribution has begun to soften
(see Fig. \ref{nest:fig:plasticweights}). Simulation for a longer period
results in an approximately Gaussian distribution of weights.

\section{Example 4: Classes and Automatization Techniques} 
So far, we have encouraged you to try our examples line-by line. This
is possible in interactive sessions, but if you want to run a
simulation several times, possibly with different parameters, it is
more practical to write a script that can be loaded from Python.

Python offers a number of mechanisms to structure and organize not
only your simulations, but also your simulation data.  The first step
is to re-write a model as a \emph{class}. In Python, and other
object-oriented languages, a class is a data structure which groups
data and functions into a single entity. In our case, data are the
different parameters of a model and functions are what you can do with
a model.
Classes allow you to solve various common problems in simulations:
\begin{description}
\item[\bf Parameter sets] Classes are data structures and so are
  ideally suited to hold the parameter set for a model. Class inheritance
  allows you to modify one, few, or all parameters while maintaining
  the relation to the original model.
\item[\bf Model variations] Often, we want to change minor aspects of
  a model. For example, in one version we have homogeneous connections
  and in another we want randomized weights. Again, we can use class
  inheritance to express both cases while maintaining the conceptual
  relation between the models.
\item[\bf Data management] Often, we run simulations with different
  parameters, or other variations and forget to record which data file
  belonged to which simulation. Python's class mechanisms provide a
  simple solution.
\end{description}
We organize the model from Section \ref{nest:sec:brunel} into a class,
by realizing that each simulation has five steps which can be factored
into separate functions:
\begin{enumerate}
\item Define all independent parameters of the model. Independent
  parameters are those that have concrete values which do not depend
  on any other parameter. For example, in the Brunel model, the
  parameter $g$ is an independent parameter.
\item Compute all dependent parameters of the model. These are all
  parameters or variables that have to be computed from other
  quantities (e.g. the total number of neurons).
\item Create all nodes (neurons, devices, etc.)
\item Connect the nodes.
\item Simulate the model.
\end{enumerate}
We translate these steps into a simple class layout that will fit
most models:
\renewcommand*\thelstnumber{c\arabic{lstnumber}}
\begin{lstlisting}[name=classes]
class Model(object):
    """Model description."""
    # Define all independent variables.

    def __init__(self):
        """Initialize the simulation, setup data directory"""
    def calibrate(self): !\label{nest:ln:calibrate}!
        """Compute all dependent variables"""
    def build(self):     !\label{nest:ln:build}!
        """Create all nodes"""
    def connect(self):   !\label{nest:ln:connect}!
        """Connect all nodes"""
    def run(self,simtime):!\label{nest:ln:run}!
        """Build, connect and simulate the model"""
\end{lstlisting}
In the following, we illustrate how to fit the model from Section
\ref{nest:sec:brunel} into this scaffold. The complete and commented
listing can be found in your NEST distribution.

\begin{lstlisting}[name=brunel-classes]
class Brunel2000(object): !\label{nest:ln:classdef}!
    """ !\label{nest:ln:docstring}!
    Implementation of the sparsely connected random network, 
    described by Brunel (2000) J. Comp. Neurosci.  
    Parameters are chosen for the asynchronous irregular
    state (AI).
    """
    g     = 5.0 
    eta   = 2.0
    delay = 1.5
    tau_m = 20.0
    V_th  = 20.0
    N_E   = 8000
    N_I   = 2000
    J_E   = 0.1
    N_rec   = 50
    threads=2      # Number of threads for parallel simulation
    built=False    # True, if build() was called
    connected=False# True, if connect() was called
    # more definitions follow...
\end{lstlisting}
A Python class is defined by the keyword \lstinline!class! followed by
the class name, \lstinline!Brunel2000! in this example. The parameter
\lstinline!object! indicates that our class is a subclass of a general
Python Object. After the colon, we can supply a documentation string,
encased in triple quotes, which will be printed if we type
\lstinline{help(Brunel2000)}.  After the documentation string, we
define all independent parameters of the model as well as some global
variables for our simulation. We also introduce two Boolean variables
\lstinline!built!  and \lstinline!connected! to ensure that the
functions \lstinline!build()! and \lstinline!connect()! are executed
exactly once.

Next, we define the class functions. Each function has at least the
parameter \lstinline!self!, which is a reference to the
current class object. It is used to access the functions and
variables of the object.

The first function we look at is also the first one that is called for
every class object.  It has the somewhat cryptic name
\lstinline!__init__()!:
\begin{lstlisting}[name=brunel-classes]
    def __init__(self):
        """
        Initialize an object of this class.
        """
        self.name=self.__class__.__name__ !\label{nest:ln:self.name}!
        self.data_path=self.name+'/'
        nest.ResetKernel()
        if not os.path.exists(self.data_path):!\label{nest:ln:checkpath}!
            os.makedirs(self.data_path)!\label{nest:ln:makepath}!
        print "Writing data to: "+self.data_path
        nest.SetKernelStatus({'data_path': self.data_path})
\end{lstlisting}

\lstinline!__init__()! is automatically called by Python whenever a
new object of a class is created and before any other class function
is called. We use it to initialize the NEST simulation kernel and to
set up a directory where the simulation data will be stored.

The general idea is this: each simulation with a specific parameter
set gets its own Python class. We can then use the class name to
define the name of a data directory where all simulation data are
stored.

In Python it is possible to read out the name of a class from an
object. This is done in line \ref{nest:ln:self.name}. Don't worry
about the many underscores, they tell us that these names are provided
by Python. In the next line, we assign the class name plus a trailing
slash to the new object variable \lstinline!data_path!. Note how all
class variables are prefixed with \lstinline!self!. 

Next we reset the NEST simulation kernel to remove any leftovers from
previous simulations.

The following two lines use functions from the Python library
\lstinline!os! which provides functions related to the operating
system. In line \ref{nest:ln:checkpath} we check whether a directory
with the same name as the class already exists. If not, we create a
new directory with this name.  Finally, we set the data path property
of the simulation kernel. All recording devices use this location to
store their data. This does not mean that this directory is
automatically used for any other Python output functions. However,
since we have stored the data path in an object variable, we can use
it whenever we want to write data to file.

The other class functions are quite straightforward. 
\lstinline!Brunel2000.build()! accumulates all commands that
relate to creating nodes. The only addition is a piece of code that
checks whether the nodes were already created:
\begin{lstlisting}[name=brunel-classes]
    def build(self):
        """
        Create all nodes, used in the model.
        """
        if self.built: return
        self.calibrate()
        # remaining code to create nodes
        self.built=True
\end{lstlisting}  
The last line in this function sets the variable
\lstinline!self.built! to \lstinline!True! so that other functions
know that all nodes were created.

In function \lstinline!Brunel2000.connect()! we first ensure that all nodes are
created before we attempt to draw any connection:
\begin{lstlisting}[name=brunel-classes]
    def connect(self):
        """
        Connect all nodes in the model.
        """
        if self.connected: return
        if not self.built:
            self.build()
        # remaining connection code
        self.connected=True
\end{lstlisting}
Again, the last line sets a variable, telling other functions that the
connections were drawn successfully.

\lstinline!Brunel2000.built! and \lstinline!Brunel2000.connected! are
state variables that help you to make dependencies between functions
explicit and to enforce an order in which certain functions are
called. The main function \lstinline!Brunel2000.run()! uses both
state variables to build and connect the network:
\begin{lstlisting}[name=brunel-classes]
    def run(self, simtime=300):
        """
        Simulate the model for simtime milliseconds and print the
        firing rates of the network during this period.  
        """
        if not self.connected:
            self.connect()
        nest.Simulate(simtime)
        # more code, e.g. to compute and print rates
\end{lstlisting}
In order to use the class, we have to load the file with the class
definition and then create an object of the class:
\begin{lstlisting}[numbers=none]
from brunel2000_classes import *
net=Brunel2000()
net.run(500)
\end{lstlisting} 
Finally, we demonstrate how to use Python's class inheritance to
express different parameter configurations and versions of a model.
In the following listing, we derive a new class that simulates a
network where excitation and inhibition are exactly balanced,
i.e. $g=4$:
\begin{lstlisting}[name=brunel-classes]
class Brunel_balanced(Brunel2000):
    """
    Exact balance of excitation and inhibition
    """
    g=4
\end{lstlisting}
Class \lstinline!Brunel_balanced! is defined with class
\lstinline!Brunel2000! as parameter. This means the new class inherits
all parameters and functions from class \lstinline!Brunel2000!. Then,
we redefine the value of the parameter \lstinline!g!. When we create
an object of this class, it will create its new data directory. 

We can use the same mechanism to implement alternative version of the
model. For example, instead of re-implementing the model with
randomized connection weights, we can use inheritance to change just the way
nodes are connected:
\begin{lstlisting}[name=brunel-classes]
class Brunel_randomized(Brunel2000):
    """
    Like Brunel2000, but with randomized connection weights.
    """
    def connect(self):
        """
        Connect nodes with randomized weights.
        """
        # Code for randomized connections follows
\end{lstlisting}
Thus, using inheritance, we can easily keep track of different
parameter sets and model versions and their associated simulation
data. Moreover, since we keep all alternative versions, we also have a
simple versioning system that only depends on Python features, rather
than on third party tools or libraries.
The full implementation of the model using classes can be found in the
examples directory of your NEST distribution.

\section{How to continue from here}
In this chapter we have presented a step-by-step introduction to NEST,
using concrete examples. The simulation scripts and more examples are
part of the examples included in the NEST distribution. 
Information about individual PyNEST functions can be obtained with
Python's \lstinline!help()!\index{NEST!help} function. For example:
\begin{lstlisting}[numbers=none,basicstyle=\footnotesize, language=tex]
>>>help(nest.Connect)

Connect(*args, **kwargs)
    Connect pre neurons to post neurons.
    
    Neurons in pre and post are connected using the specified connectivity
    (one-to-one by default) and synapse type (static_synapse by default).
    Details depend on the connectivity rule.
  
    ...  
 \end{lstlisting}

To learn more about NEST's node and synapse types, you can access
NEST's help system, using the PyNEST command NEST's online help still
uses a lot of SLI syntax, NEST's native simulation language. However
the general information is also valid for PyNEST.

Help and advice can also be found on NEST's user mailing list where
developers and users exchange their experience, problems and ideas.
And finally, we encourage you to visit the web site of the NEST
Initiative at \url{www.nest-initiative.org}.

\section*{Acknowledgements}
AM partially funded by BMBF grant 01GQ0420 to BCCN Freiburg, Helmholtz
Alliance on Systems Biology (Germany), Neurex, and the Junior
Professor Program of Baden-W\"{u}rttemberg. HEP partially supported by
RCN grant 178892/V30 eNeuro. HEP and MOG were partially supported by EU
grant FP7-269921 (BrainScaleS).


\appendix
\section*{Version information}
The examples in this chapter were tested with the following versions.\\
NEST: pre-2.6.0 (r11744), Python: 2.7.8, Matplotlib: 1.3.1, NumPy: 1.8.1.

\bibliography{NEST_By_Example}
\bibliographystyle{plainnat}

\printindex

\end{document}
